{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classification model\n",
    "\n",
    "First, reload in the dataset with as many images as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PersonDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annot_path, img_dir, img_dim, categories):\n",
    "        self.annot_path = annot_path\n",
    "        self.img_dir =  img_dir\n",
    "        self.img_dim =  img_dim\n",
    "        self.categories =  categories\n",
    "\n",
    "        self.img_data_all, self.gt_bboxes_all, self.gt_classes_all = self.get_data()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_data_all)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.img_data_all[index], self.gt_bboxes_all[index], self.gt_classes_all[index]\n",
    "    \n",
    "    def get_data(self):\n",
    "        max_objects = 10\n",
    "        \n",
    "        #get_boxes_all has shape (B, n_images, max_objects, 4) where B is batch size\n",
    "        df_images = filter_aspect_ratio(self.annot_path, 1.6, 1.4)\n",
    "        gt_boxes_all, gt_idxs_all, img_data_all = load_dataset(df_images, self.annot_path, self.img_dir, max_objects, self.img_dim)\n",
    "\n",
    "        gt_bboxes_pad = pad_sequence(gt_boxes_all, batch_first=True, padding_value=-1) #pad if there are images with < max_objects\n",
    "        gt_classes_pad = pad_sequence(gt_idxs_all, batch_first=True, padding_value=-1)\n",
    "\n",
    "        img_data_stacked = torch.stack(img_data_all, dim=0)\n",
    "\n",
    "        print('total length of image data is', len(img_data_stacked))\n",
    "        \n",
    "        return img_data_stacked.to(dtype=torch.float32), gt_bboxes_pad, gt_classes_pad\n",
    "img_width = 600\n",
    "img_height = 400\n",
    "name2idx = {'pad': -1, 'person': 1}\n",
    "categories = {v:k for k, v in name2idx.items()}\n",
    "\n",
    "annotation_path = './mmdetection/data/coco/annotations/instances_train2017.json'\n",
    "image_dir = './mmdetection/data/coco/train2017/'\n",
    "od_dataset = PersonDataset(annotation_path, image_dir, (img_height, img_width), categories)\n",
    "od_dataloader = DataLoader(od_dataset, batch_size=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in the model and define a training loop, plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "out_c, out_h, out_w = (2048, 13, 19)\n",
    "width_scale_factor = img_width // out_w\n",
    "height_scale_factor = img_height // out_h\n",
    "height_scale_factor, width_scale_factor\n",
    "\n",
    "img_size = (img_height, img_width)\n",
    "out_size = (out_h, out_w)\n",
    "n_classes = len(name2idx) - 1 # exclude pad idx\n",
    "roi_size = (2, 2)\n",
    "\n",
    "detector = Detector(img_size, out_size, out_c, n_classes, roi_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    \n",
    "    for i in tqdm(range(n_epochs)):\n",
    "        total_loss = 0\n",
    "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
    "            \n",
    "            # forward pass\n",
    "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
    "            \n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        loss_list.append(total_loss)\n",
    "        \n",
    "    return loss_list\n",
    "\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 100\n",
    "\n",
    "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)\n",
    "\n",
    "plt.plot(loss_list)\n",
    "torch.save(detector.state_dict(), \"model_5.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6473775a3378ccec2f5e00dbe341aba7fdee7c702eff9359ac1625be8f78b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
